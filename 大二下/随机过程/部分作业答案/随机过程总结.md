# 5.8 一致化和部分习题解答

$$
在本章节中，我们考虑的是一切状态的平均时间都相同的Marvkov链，记N(t)为时间t为止的转移次数，则N(t)为速率为v的Poisson过程。\\
以P_{ij}(t)举例，取N(t)为条件：\\
P_{ij}(t)=P(X(t)=j|X(0)=i)\\
= \sum _{n=0}^{\infty}P(X(t)=j|X(0)=i,N(t)=n)P(N(t)=n|X(0)=i)\\
由 速率为v的Poisson过程\\
= \sum _{n=0}^{\infty}P(X(t)=j|X(0)=i,N(t)=n)\frac {e^{-vt}(vt)^n}{n !}\\
\because N(t)为条件\\
\therefore 我们已知n次徘徊到j状态，但是因为在每个状态的分布是相同的，没有其他信息。\\
故P(X(t)=j|X(0)=i,N(t)=n)=P^n_{ij}\\
代入可得\\
P_{ij}(t)=\sum_{n=0}^\infty P_{ij}^n\frac {e^{-vt}(vt)^n}{n !}\; 5.8.1\\
至此，我们得到了部分和拟合P_{ij}(t)，我们可以用转移概率矩阵的成绩来计算n步的转移概率P_{ij}^n\\
为了进一步探究5.8.1的应用，我们先让v_i上限是v，不再直接令其相等，运用虚拟转移技巧。\\
我们将转移的过程即以v_i速率离开，等价于假设转移以速率v发生，其中比例v_i/v是实际离开i的转移，而其余部分都是仍留在i的虚拟转移\\
用简练的语句来说就是，在假设v_i\le v的Markov 过程，可以这么理解\\
它以速率v的指数时间总量停留在状态i，然后以概率P^{*}_{ij}进行一次转移到j的过程，其中\\
P^*_{ij}=\left\{\begin{matrix}
 1-\frac{v_i}{v},j=i \\ \frac{v_i}{v}P_{ij},j\ne i 

\end{matrix}\right.
\\
进而导出5.8.1中的P_{ij}(t)=\sum _0^\infty P_{ij}^{*n}\frac {e^{-vt}(vt)^n}{n !}\\
其中，我们的一致化操作就是这种用引入从一个状态到各自状态的转移使每个状态发生速率一致的技巧。
$$

$$
我们了解了一致化操作后，重新考虑Kolmogorov5.4(A)中介绍的两状态链，初始化\\
P_{01}=P_{10}=1,\\
v_0=\lambda, \; v_1=\mu.\\
let \;\;v =\lambda+\mu\;\; 在我们使用一致化思想后\\
P_{00}=\frac{\mu}{\lambda+\mu}=1-P_{01}\\
P_{10}=\frac{\mu}{\lambda+\mu}=1-P_{11}\\
考虑初始状态如此的Markov链\\
\because P_{00}=P_{10}，由此推出无论当前状态为何都是，回到状态0的概率相等，均为\frac{\mu}{\mu+\lambda}\\
\therefore P_{00}(t)=\sum _{n=0}^{\infty} P^n_{00}e^{-(\lambda+\mu)t}\frac{[(\lambda+\mu)t]^n}{n!}=e^{-(\lambda+\mu)t}+[1-e^{-(\lambda+\mu)t}]\frac{\mu}{\mu+\lambda}=\frac{\lambda}{\mu+\lambda}e^{-(\lambda+\mu)t}+\frac{\mu}{\mu+\lambda}\\
我们能类似地推出\\
P_{11}(t)=\sum_{n=0}^{\infty}P_{11}^n e^{-(\lambda+\mu)t}\frac{[(\lambda+\mu)t]^n}{n!}=\frac{\lambda}{\mu+\lambda}+\frac{\mu}{\mu+\lambda}e^{-(\lambda+\mu)t}\\
除了转移概率P，考虑前章节提到的一个状态量X(t)直至时刻t为止过程处在一个指定状态的全部时间\\
令S_1(t)=\int_0^tX(s)ds, S_0=\int_0^t(1-X(s))ds\;\;5.8.4\\
可以从定义中看出，S_i(t)表示到t为止在状态i的时间，随机过程S_i(t)称为对状态i的占有时间。\\
在本例子中，i=0,1\\
现在假设X(0)=0，我们确定S_0(t)的分布，用5.8.4计算其期望均值\\
E[S_0(t)|X(0)=0]=\int_0^tE[1-X(s)]ds=P_{00}(s)ds=\frac{\mu}{\mu+\lambda}t+\frac{\lambda}{(\mu+\lambda)^2}[1-e^{-(\lambda+ \mu)t}] \\
在这部分，将考虑S_0(t)的分布\\
在同一个初始条件X(0)=0,S_0(t)的条件分布，同样的，取N(t)条件，对s<t\\
P(S_0(t)\le s)=\sum _{n=0}^\infty e^{-(\lambda+\mu)t} \frac{[(\lambda+\mu)t]^n}{n!} P(S_0(t)\le s|N(t)=n)\\
\because N(t)=0->S_0(t)=t,X(0)=0\\
=\sum _{n=1}^\infty e^{-(\lambda+\mu)t} \frac{[(\lambda+\mu)t]^n}{n!} P(S_0(t)\le s|N(t)=n)\\
对N(t)=n时,\\
(0,t)分为n+1个区间，(0,X_{(1)}),(X_{(1)},X_{(2)}),...,(X_{(n)},t),这里X_{(i)}指的是到t为止过程<N(h)>的第i个事件的时刻\\
其中，在任意指定的子区间过程有<X(s)>将在相同状态，它将在第一个子区间处于状态0，且在以后的每个子区间独立地以概率\frac{\mu}{\mu+\lambda}\\
处于0状态。\\
初始化状态0，本流程的第一个区间将处于状态0，而且根据转移概率，在之后每个区间内都是以\frac{\mu}{\mu+\lambda}\\
简化来说，在给定N(t)=n的条件下，过程是0的子区间个数可以用1+B(n,\mu/(\mu+\lambda))表示。\\
$$

$$
若上述提到的子区间个数之和为k，则S_0(t)将等于上述k个子区间长度的和。\\
因为X_{(1)},...,X_{(n)}和一组n个独立的(0,t)均匀随机变量的次序统计量有相同的分布，由此推导出n+1个子区间长度Y_1,...Y_n\\
的联合分布是可交换的。即P(Y_i\le y_i,i=1,...,n+1)是向量(y_1,...,y_{n+1})的对称函数。\\
因为任意k个Y_i的和的分布是相同的，X_{(k)}=Y_1+...+Y_k\\
\therefore P(X_{(k)}\le s)=\sum_{i=k}^n\binom{n}{i} (\frac{s}{t})^i(1-\frac{s}{t})^{n-i}\;\;\;二项分布可得\\
综上,s<t有\\
P(S_0(t)\le s |X(0)=0)\\
=\sum_{n=1}^\infty e^{-(\lambda+\mu)t}\frac {((\mu+\lambda)t)^n}{n !}\sum_{k=1}^n\binom{n}{k-1}(\frac{\mu}{\mu+\lambda})^{k-1}(\frac{\lambda}{\mu+\lambda})^{n-k+1}\times \sum_{i=k}^n \binom{n}{j}(\frac{s}{t})^{i}(1-\frac{s}{t})^{n-i}\\
and \;\; P(S_0(t)=t|X(0)=0)=e^{-\lambda t }
$$

# eg

$$
5.1\\
q[(n, m), (n+1, m)] = q[(n,m),(n, m+1)] = \frac{\lambda mn}{2}\\
5.2\\
q[(n, m), (n-1, m+1)] = \alpha n\\
q[(n, m), (n+2, m-1)] = \beta m\\
5.4\\
设T_i独立，表示i-1和i之间的时间，其符合\lambda_i的指数分布\\
解为\\
E[\sum_{i=1}^{n}T_i|n = N-1] = \sum_{i=1}^{N-1} E[T_i] = \sum_{i=1}^{N-1} 1/\lambda_i \\
Var(\sum_{i=1}^{n}T_i|n = N-1 ) = \sum_{i=1}^{N-1} Var(T_i) = \sum_{i=1}^{N-1} 1/\lambda_i^2 \\
\phi(t) = \prod_{i=1}^{N-1}\frac{\lambda_i}{\lambda_i – t}\\
5.6\\
将此视为续订奖励过程，每个人每单位时间获得单位支付，那么成本的总和也是年龄的总和。\\
A(t) = a_0 + t + \int_0^t X(s) – 1 ds = a_0 + \int_0^t X(s)ds\\
5.9\\
P_{ij}(t+s) = P\{X(t+s) = j | X(0) = i\} \\
= \sum_{k=0}^{\infty} P\{X(t+s) = j, X(t) = k | X(0) = i\} \\
= \sum_{k=0}^{\infty} P\{X(t+s) = j| X(t) = k, X(0) = i\}P\{X(t) = k | X(0) = i\}\\ 
= \sum_{k=0}^{\infty} P\{X(t+s) = j| X(t) = k\}P\{X(t) = k | X(0) = i\}\\  
= \sum_{k=0}^{\infty} P_{kj}(s)P_{ik}(t)\\  
5.10\\
(a)\\
\lim_{t \to 0}\frac{1 – P(t)}{t} = \lim_{t \to 0}\frac{1 – P(t)}{t} = v_0\\
(b)\\
P(t)P(s) \leq \sum_{k=0}^{\infty}P_{0k}(t)P_{k0}(s) = P(t+s)\\
= \sum_{k=1}^{\infty}P_{0k}(s)P_{k0}(t) + P_{00}(t)P_{00}(s) \\
\leq \sum_{k=0}^{\infty}P_{0k}(s) + P(s)P(t) \\
= 1 – P(s) + P(s)P(t) \\
(c)\\
由(b)\\
P(s)P(t-s) \leq P(t) \leq 1 – P(t-s) + P(t-s)P(s)\\
P(s)(P(t-s) – 1) \leq P(t) – P(s) \leq (1 – P(s))(1 – P(t-s)) \\
P(t-s) – 1 \leq P(t) – P(s) \leq (1 – P(t-s) \\
5.11\\
(a)\\
P_{ij}^{\prime}(t) = {j-1 \choose i-1} \lambda e^{-i\lambda t} [-i(1 – e^{-\lambda t})^{j-i} + (j-i)e^{-\lambda t}(1 – e^{-\lambda t})^{j-i-1}] \\
= {j-1 \choose i-1} \lambda e^{-i\lambda t}(1 – e^{-\lambda t})^{j-i-1}(je^{-\lambda t} – i)\\
\sum_{k \neq i} q_{ik}P_{kj}(t) – v_iP_{ij}(t) \\
=q_{i,i+1}P_{i+1, j}(t) – v_iP_{ij}(t) \\
=i\lambda[{j – 1 \choose i – 2}e^{-i\lambda t}e^{-\lambda t}(1 – e^{-\lambda t})^{j-i -1} – {j – 1 \choose i – 1}e^{-i\lambda t}(1 – e^{-\lambda t})^{j-i}]\\
=i\lambda e^{-i\lambda t}(1 – e^{-\lambda t})^{j-i-1} [{j-1 \choose i}e^{-\lambda t} – {j-1 \choose i – 1}(1 – e^{-\lambda t})] \\ 
={j-1 \choose i-1} \lambda e^{-i\lambda t}(1 – e^{-\lambda t})^{j-i-1}(je^{-\lambda t} – i)\\
\sum_{k \neq j} q_{kj}P_{ik}(t) – v_jP_{ij}(t) \\
=q_{j-1,j}P_{i, j-1}(t) – v_jP_{ij}(t) \\
=\lambda[(j-1){j – 2 \choose i – 1}e^{-i\lambda t}(1 – e^{-\lambda t})^{j-1-i} – j{j – 1 \choose i – 1}e^{-i\lambda t}(1 – e^{-\lambda t})^{j-i}]\\
=\lambda e^{-i\lambda t}(1 – e^{-\lambda t})^{j-i-1} [(j-1){j-2 \choose i – 1} – j{j-1 \choose i – 1}(1 – e^{-\lambda t})] \\ 
={j-1 \choose i-1} \lambda e^{-i\lambda t}(1 – e^{-\lambda t})^{j-i-1}(je^{-\lambda t} – i)\\ 
(b)\\
P\{\tau = t\} = \sum_{n=1}^{\infty} P\{\tau = t | X(T) = n\}P\{X(T) = n\} \\
= \sum_{n=1}^{\infty} e^{-\mu t}\frac{(\mu t)^{n-1}}{(n-1)!}e^{-\lambda T}(1 – e^{-\lambda T})^{n-1} \\
设t_i表示在i-1和i出发之间的时间\\
E[\tau] = E[\sum_{i=1}^{X(T)} t_i] = E[X(T)]E[t_i] = e^{\lambda T}/\mu \\
$$

$$
 定义续展奖励流程如下。当进程进入状态 0 并且周期中的奖励等于该周期中的事件数时，将发生续订。让第n个周期的长度X_n\\
是在状态 0 和 1 中花费的时间的总和，例如X_{0n}和X_{1n}分别。因此\\
E[X_n] = E[X_{0n}] + E[X_{1n}] = \lambda^{-1} + \mu^{-1}\\
如果R_n是第n个周期中的奖励，与R_{0n}和R{1n}分别在状态 0 和 1 中赚取\\
\begin{align}
E[R_n] &= E[R_{0n}] + E[R_{1n}] = E[E[R_{0n}| X_{0n}] + E[R_{1n}|X_{1n}]] \\
&= E[\alpha_0X_{0n}] + E[\alpha_1X_{1n}]\\
&= \alpha_0/\lambda + \alpha_1/\mu.
\end{align}\\
\therefore \lim_{t \to \infty} \frac{N(t)}{t} = \lim_{t \to \infty} \frac{R(t)}{t} = \frac{E[R]}{E[X]} = \frac{\alpha_0\mu + \alpha_1\lambda}{\lambda + \mu}\\
(b)\\
让T_i(t)表示在状态i下花费的时间t\\

E[N(t)] = \alpha_0E[T_0(t)] + \alpha_1(1 – E[T_0(t)])\\
= (\alpha_0 – \alpha_1)E[T_0(t)] + \alpha_1t \\
= \alpha_1t + (\alpha_0 – \alpha_1)\int_0^t P_{00}(s)ds
\\
通过正向方程，P_{00}^{\prime} = – (\lambda + \mu)P_{00}(t) + \mu\\
解得\\

P_{00}(t) = \frac{\mu}{\lambda + \mu} + \frac{\lambda}{\lambda + \mu}e^{-(\lambda + \mu)t} \\
E[N(t)] = \frac{\alpha_0\mu + \alpha_1\lambda}{\lambda + \mu}t + \frac{\alpha_1 – \alpha_0}{(\lambda + \mu)^2}\lambda(e^{-(\lambda + \mu)t} – 1)
\\
5.15\\
(a)\\
线性的生灭过程\\
(b)\\
\mu_n=n\mu\\
\lambda_n=n\lambda+\theta\\
(c)\\
\because E[X(t+h)|X(0)] = E[X(t)|X(0)] + (\lambda – \mu)E[X(t)|X(0)]h + \theta h + o(h)\\
let \;\;M(t) \equiv E[X(t)|X(0)]\\
M^{\prime}(t) = (\lambda – \mu)M(t) + \theta\\
To \;solve\;M(0) = i\\
M(t) = \left\{\begin{array}{ll}
\theta t + i \quad \lambda = \mu \\
(i+\frac{\theta}{\lambda – \mu})e^{(\lambda-\mu)t}-\frac{\theta}{\lambda-\mu} \quad otherwise \\ \end{array}\right. \\
$$

